{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MI_0pJi09KFE"
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install pinecone-client\n",
    "\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "!pip install pinecone\n",
    "\n",
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G_O3HoH-20t"
   },
   "source": [
    "The below sell loades the pdfs is a bit slower then just drag and drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "M-5tCAW_8Gyu"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"\\n please, uplode the files in PDF format only\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "pdf_paths = []\n",
    "\n",
    "for i in uploaded.keys():\n",
    "  i = \"/content/\"+i\n",
    "  print(i)\n",
    "  pdf_paths.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yj_KaTyHP7Tk"
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=\"pcsk_5waTvD_FuhmqHPXyhSYan4iWgFSr5D9tLoyoQqQ97NDtStNLXfcgT6cbjKdEd1BUswnE3A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aruYEetlSRA0"
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "txts = []\n",
    "for pdf_path in pdf_paths:\n",
    "  txts.append(extract_text_from_pdf(pdf_path))\n",
    "\n",
    "#print(txts[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RPqSz_j7o8lL"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "pdf_chunks = []\n",
    "for each_pdf_text in txts:\n",
    "    pdf_chunks.extend(text_splitter.split_text(each_pdf_text))\n",
    "\n",
    "print(f\"Total Chunks: {len(pdf_chunks)}\")\n",
    "#print(pdf_chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OMlaXefoqY2d"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "embeddings = model.encode(pdf_chunks, show_progress_bar=True)\n",
    "\n",
    "#print(f\"First txt Embedding: {embeddings[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "tAEiJgh41AFD"
   },
   "outputs": [],
   "source": [
    "vectors = [\n",
    "      (\n",
    "        str(i), embeddings[i], {\"text\": pdf_chunks[i]}\n",
    "      )\n",
    "\n",
    "      for i in range(len(pdf_chunks))\n",
    "]\n",
    "\n",
    "print(f\"First ID: {vectors[0][0]}\")\n",
    "\n",
    "print(f\"Last ID: {vectors[-1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNaGNHgtyL1h"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "size = 0\n",
    "for vector in vectors :\n",
    "    size += sys.getsizeof(vector)\n",
    "\n",
    "print(f\"Total size of the request: {size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGUyGbwzTkkq"
   },
   "outputs": [],
   "source": [
    "index_name = \"hondacars\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "tImh2E9-Vs4Y"
   },
   "outputs": [],
   "source": [
    "index = pc.Index(index_name)\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "splitted_vectors = []\n",
    "def batch_data(vectors, batch_size):\n",
    "\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        splitted_vectors.append(vectors[i:i + batch_size])\n",
    "    return splitted_vectors\n",
    "\n",
    "batches = batch_data(vectors, batch_size)\n",
    "\n",
    "for batch in batches:\n",
    "    index.upsert(batch)\n",
    "    print(f\"Upserted batch of size {len(batch)}\")\n",
    "\n",
    "print(\"Embeddings stored in Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "89laGLN760jR"
   },
   "outputs": [],
   "source": [
    "def query_pinecone(query, top_k=3):\n",
    "\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    query_embedding = query_embedding.tolist()\n",
    "\n",
    "    results = index.query(vector=[query_embedding], top_k=top_k, include_metadata=True)\n",
    "    # print(results)\n",
    "\n",
    "    relevant_chunks = []\n",
    "    for matchh in results['matches']:\n",
    "      relevant_chunks.append(matchh ['metadata']['text'])\n",
    "\n",
    "    txt = \" \".join(relevant_chunks)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Og2tJYI8KPUI"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "def generate_ans(relevent_txt, question):\n",
    "    context = relevent_txt\n",
    "    given_prompt = f\"Context: {context}\\nQuestion: {question}\"\n",
    "\n",
    "    generator = InferenceClient(model='meta-llama/Llama-3.2-3B-Instruct' , timeout=120)\n",
    "\n",
    "    generated_respo = generator.post(\n",
    "        json={\n",
    "            'inputs' : given_prompt,\n",
    "            'parameters':{'max_new_tokens' : 200},\n",
    "            'task' : 'text-generation'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    res = json.loads(generated_respo.decode())[0][\"generated_text\"]\n",
    "\n",
    "    res = res[len(question):]\n",
    "    res = res[len(context):]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVi0g0gCV8Xt"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxHh2zH6BFhj"
   },
   "source": [
    "give me motorcycle business of 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTkO0DEKyMsb"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def get_rag_answer(user_question):\n",
    "    txt = query_pinecone(user_question)\n",
    "\n",
    "    ress = generate_ans(txt,user_question)\n",
    "    print(ress)\n",
    "    return ress\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn = get_rag_answer,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG QA Business Bot\",\n",
    "    description=\"Ask me anything about the pdfs you just droped\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
